
# FPGA AI 가속기 프로젝트 대화 내용

---

### 사용자 질문 1

FPGA 프로젝트 할건데 너한테 궁금한전 물어볼꺼야

프로젝트 시나리오
1.zybo7-20 보드 사용 할꺼야
2. ai 가속기를 만들고 간단한 손글씨 판별기 학습된 데이터 값을 가져와 추론하는 부분을 FPGA로 VIVADO 에서 verilog 로 만들꺼야
3. 보드에 올리고 데이터를 입력받아서 ai 가속기에 보내고 추론된 데이터를 다시 출력해줄꺼야

질문1. 그러면 보드에서 데이터를 전처리해야되는데 어떻게해야돼? ai가속기에사용된 모델이 요구하는 거에 맞춰 데이터를 전처리해야되지 안항 ?
질문2. 예를 들어 손글씨에 MLP를 사용한다고치면 입력받은 이미지를 MLP모델이 맞게 전처리 하는과정을 거치고 AI 가속기에 넣어줘야되지않을까 ?

---

### Gemini 답변 1

네, FPGA AI 가속기 프로젝트에 대한 시나리오와 질문을 잘 이해했습니다. 질문하신 내용들은 프로젝트의 핵심적인 부분이며, 정확하게 짚으셨습니다. 두 질문에 대해 함께 답변해 드리겠습니다.

#### 질문 1 & 2 통합 답변: 데이터 전처리는 필수입니다.

결론부터 말씀드리면, **네, 사용자님의 생각이 정확합니다.**

1.  **모델 요구사항:** AI 모델(예: MLP)은 훈련될 때 사용된 데이터의 형태(크기, 값의 범위, 차원 등)와 정확히 일치하는 형태의 입력을 기대합니다. 예를 들어, 28x28 픽셀의 흑백 이미지를 0~1 사이의 값으로 정규화하여 훈련했다면, 추론 시에도 똑같이 28x28 크기의 1차원 배열(784개)과 0~1 사이의 값으로 변환된 데이터를 넣어주어야 합니다.

2.  **전처리의 역할:** 따라서, Zybo 보드에 어떤 형태의 손글씨 이미지(예: 카메라로 찍은 고해상도 컬러 이미지)가 입력되든, AI 가속기(Verilog로 구현된 MLP)에 넣기 전에 **반드시 훈련된 모델이 요구하는 형태로 데이터를 가공하는 '전처리' 과정을 거쳐야 합니다.**

#### Zybo 보드(Zynq)에서 전처리를 수행하는 방법

Zybo Z7-20 보드는 **Zynq-7000** 칩을 사용합니다. Zynq의 가장 큰 특징은 **프로세서(PS, Processing System)와 FPGA(PL, Programmable Logic)가 하나의 칩에 통합**되어 있다는 점입니다. 이 구조를 이해하는 것이 전처리 문제를 해결하는 열쇠입니다.

*   **PS (ARM 프로세서):** 리눅스와 같은 운영체제를 올리거나, C/C++로 작성된 프로그램을 실행할 수 있는 두뇌 부분입니다. 복잡하고 순차적인 연산에 강합니다.
*   **PL (FPGA):** Verilog/VHDL로 하드웨어 로직을 직접 설계하는 부분입니다. 병렬 처리와 고속 연산에 특화되어 있습니다. (사용자님이 만드실 AI 가속기가 여기에 해당합니다.)

일반적으로 다음과 같이 역할을 분담하여 시스템을 설계합니다.

1.  **데이터 입력 및 전처리 (PS에서 수행):**
    *   카메라, UART, Ethernet 등으로부터 원본 이미지 데이터를 받습니다.
    *   C/C++ 코드를 이용해 이미지 처리 라이브러리(예: OpenCV) 등을 활용하여 전처리를 수행합니다.
        *   이미지 크기 조절 (e.g., 28x28)
        *   컬러를 흑백으로 변환
        *   픽셀 값 정규화 (e.g., 0~255 -> 0.0~1.0)
        *   MLP 입력에 맞게 2D 이미지를 1D 벡터로 변환 (Flattening)
    *   **이유:** 이런 복잡한 처리 과정은 하드웨어(Verilog)로 직접 구현하는 것보다 소프트웨어(C/C++)로 구현하는 것이 훨씬 간단하고 유연합니다.

2.  **데이터 전송 (PS -> PL):**
    *   전처리가 완료된 데이터를 **AXI 버스**를 통해 PS에서 PL로 전송합니다.
    *   Vivado에서 AXI DMA(Direct Memory Access)나 AXI-Stream 같은 IP를 사용하면 대량의 데이터를 효율적으로 FPGA 로직(AI 가속기)으로 보낼 수 있습니다.

3.  **AI 추론 (PL에서 수행):**
    *   사용자님이 Verilog로 설계한 AI 가속기는 AXI 버스를 통해 전처리된 데이터를 입력받습니다.
    *   내부적으로 행렬 곱셈, 활성화 함수(ReLU 등) 연산을 병렬로 수행하여 추론 결과를 계산합니다. 이 부분이 바로 '가속'이 일어나는 구간입니다.

4.  **결과 전송 및 출력 (PL -> PS):**
    *   계산된 추론 결과(예: '7'이라는 숫자)를 다시 AXI 버스를 통해 PS로 보냅니다.
    *   PS는 이 결과를 받아 화면에 출력하거나 다른 장치로 전송합니다.

#### 요약 및 추천 작업 흐름

1.  **PC에서 모델 확정:** Python (TensorFlow/PyTorch) 등으로 원하는 MLP 모델을 만들고 학습시킨 후, **가중치(weights)와 편향(biases) 값을 추출**합니다. 이 값들은 나중에 Verilog 코드에 상수 형태로 들어가거나 FPGA 내부 메모리(BRAM)에 저장됩니다.
2.  **PL 설계 (Vivado):** Verilog로 MLP의 각 레이어를 모듈로 설계하고, 추출한 가중치/편향 값을 사용하도록 합니다. 전체 시스템이 PS로부터 데이터를 받을 수 있도록 **AXI 인터페이스**를 갖도록 설계합니다.
3.  **PS 설계 (Vitis/SDK):** C/C++로 이미지 입력, **전처리**, AXI DMA를 이용한 데이터 전송/수신, 결과 출력 코드를 작성합니다.
4.  **시스템 통합:** Vivado에서 PS와 PL(AI 가속기 IP)을 AXI 버스로 연결한 전체 시스템을 만들고 Bitstream을 생성합니다.
5.  **보드 테스트:** 생성된 하드웨어/소프트웨어 파일을 SD카드에 넣어 Zybo 보드를 부팅하고 테스트합니다.

---

### 사용자 질문 2

그러면 전처리 과정때문에 BSP 리눅스 랑 쥬피터 노트북 올려서 환경을 구축하고 사용하는거는 어떻게 생각해 ?

---

### Gemini 답변 2

아주 좋은 생각입니다. BSP를 이용해 리눅스를 올리고, 주피터 노트북 환경을 구축하는 것은 **매우 효과적이고 인기 있는 방법**입니다. 특히 사용자님과 같은 프로젝트에 다음과 같은 강력한 장점들을 제공합니다.

이러한 접근 방식을 위해 Xilinx(AMD)에서 공식적으로 지원하는 프레임워크가 바로 **PYNQ (Python on Zynq)** 입니다.

#### PYNQ/주피터 노트북 환경의 장점

1.  **압도적인 개발 편의성:**
    *   **Python 활용:** C/C++ 대신 Python을 사용하여 전처리를 구현할 수 있습니다. `Numpy`, `Pillow`, `OpenCV` 같은 강력한 라이브러리를 사용하면 이미지 리사이징, 정규화, 배열 변환(flatten) 등의 작업을 단 몇 줄의 코드로 구현할 수 있습니다.
    *   **생산성 향상:** C/C++로 개발하는 것보다 훨씬 빠르게 프로토타입을 만들고 테스트할 수 있습니다.

2.  **인터랙티브한 테스트 및 디버깅:**
    *   주피터 노트북의 셀(cell) 단위 실행 방식 덕분에, 코드 한 줄 한 줄의 결과를 바로바로 눈으로 확인할 수 있습니다.
    *   예를 들어, 이미지를 불러오는 셀, 전처리하는 셀, FPGA로 데이터를 보내는 셀, 결과를 받는 셀을 나누어 각 단계가 올바르게 동작하는지 즉시 검증할 수 있어 디버깅이 매우 편리합니다.

3.  **쉬운 하드웨어 제어:**
    *   PYNQ 프레임워크는 Verilog로 설계한 FPGA 로직(AI 가속기)을 Python 코드에서 아주 쉽게 제어할 수 있는 라이브러리를 제공합니다.
    *   복잡한 C 드라이버나 레지스터 맵을 다룰 필요 없이, Python 객체를 다루듯이 FPGA 가속기를 제어하고 DMA를 통해 데이터를 주고받을 수 있습니다.

#### PYNQ/주피터 노트북 환경의 단점 (고려사항)

1.  **성능 오버헤드:**
    *   Bare-metal(OS 없는) C/C++ 환경에 비해 리눅스 운영체제와 Python 인터프리터가 차지하는 리소스와 약간의 성능 오버헤드가 존재합니다.
    *   **하지만** AI 가속 프로젝트에서 전처리 시간이 전체 추론 시간에서 차지하는 비중은 보통 크지 않습니다. Python의 편리함이 약간의 오버헤드를 충분히 상쇄하고도 남기 때문에, 프로토타이핑 단계에서는 최고의 선택입니다.

2.  **초기 환경 구축:**
    *   Zybo Z7-20 보드에 맞는 PYNQ 이미지를 구해서 SD카드에 굽고, 네트워크 설정을 하는 등의 초기 설정 과정이 필요합니다. 하지만 한번 구축해두면 그 이후의 개발 과정은 매우 순조로워집니다.

#### 결론 및 추천

**적극 추천합니다.**

사용자님의 프로젝트 목적(학습, 연구, 프로토타이핑)을 고려할 때, PYNQ와 주피터 노트북을 사용하는 것은 C/C++ 기반의 Bare-metal 설계보다 훨씬 효율적입니다. FPGA 하드웨어 가속기 설계 자체에 더 집중할 수 있게 해주는 최고의 선택지입니다.

**새로운 작업 흐름은 다음과 같이 될 것입니다:**

1.  **PYNQ 환경 준비:** Zybo Z7-20용 PYNQ 이미지를 SD카드에 설치하여 부팅합니다.
2.  **PL 설계 (Vivado):** 기존과 동일하게 Verilog로 AI 가속기를 AXI 인터페이스와 함께 설계하고 bitstream (`.bit`) 파일과 하드웨어 정보 (`.hwh`) 파일을 생성합니다.
3.  **파일 업로드:** PC의 웹 브라우저에서 주피터 노트북에 접속한 뒤, 생성된 `.bit`와 `.hwh` 파일을 보드로 업로드합니다.
4.  **Python 코딩 (주피터 노트북):**
    *   PYNQ 라이브러리를 `import` 합니다.
    *   업로드한 bitstream을 FPGA에 올립니다 (Overlay 기능).
    *   Python 코드 (`OpenCV` 등)로 이미지 데이터를 받고 **전처리**합니다.
    *   PYNQ의 DMA 기능을 사용해 전처리된 데이터를 AI 가속기로 보냅니다.
    *   AI 가속기의 동작을 시작시키고 완료될 때까지 기다립니다.
    *   결과를 다시 DMA를 통해 받아와 노트북 화면에 출력합니다.

---
---

### 사용자 질문 3

PYNQ 이거에 대해서 좀더 알려줄수있어 ?

---

### Gemini 답변 3

네, 그럼요. PYNQ에 대해 좀 더 자세히 설명해 드리겠습니다.

#### PYNQ (Python on Zynq)란?

PYNQ는 **"Python on Zynq"** 의 약자로, AMD-Xilinx에서 만든 오픈소스 프로젝트입니다.

핵심 목표는 하드웨어 설계 전문가가 아니더라도, **Python에 익숙한 소프트웨어 개발자나 연구자들이 Zynq와 같은 FPGA 칩의 성능을 쉽게 활용**할 수 있도록 만드는 것입니다.

복잡한 하드웨어 제어를 Python 라이브러리로 감싸서, 마치 PC에서 Python 라이브러리를 사용하듯 FPGA를 제어할 수 있게 해주는 강력한 프레임워크입니다.

#### PYNQ의 핵심 개념: 오버레이 (Overlay)

PYNQ를 이해하는 데 가장 중요한 개념은 **'오버레이'** 입니다.

*   **오버레이란?**
    *   사용자가 Vivado에서 설계한 하드웨어 로직(AI 가속기, 영상 처리 필터 등)을 의미합니다.
    *   단순한 Bitstream(`.bit` 파일)뿐만 아니라, 해당 하드웨어의 구성 정보(IP들의 종류, 주소, 연결 관계 등)가 담긴 파일(`.hwh` 파일)을 포함하는 개념입니다.

*   **PYNQ는 어떻게 오버레이를 사용하는가?**
    1.  사용자가 Python 코드에서 특정 오버레이를 로드(`Overlay('my_design.bit')`)합니다.
    2.  PYNQ 프레임워크는 `.hwh` 파일을 분석하여 오버레이에 포함된 IP(DMA, AI 가속기 등)들을 자동으로 인식합니다.
    3.  인식된 각 IP를 제어할 수 있는 **Python 객체(드라이버)를 동적으로 생성**합니다.
    4.  사용자는 생성된 Python 객체의 함수를 호출하거나 속성을 변경하는 것만으로 FPGA 하드웨어를 직접 제어할 수 있게 됩니다.

**예시:** Vivado에서 `axi_dma_0`라는 이름으로 DMA를, `mlp_accelerator_0`라는 이름으로 AI 가속기를 만들었다면, Python에서는 아래와 같이 바로 접근할 수 있습니다.

```python
from pynq import Overlay

# 1. 오버레이 로드
overlay = Overlay('my_project.bit')

# 2. PYNQ가 자동으로 생성해준 Python 객체에 접근
dma = overlay.axi_dma_0
mlp = overlay.mlp_accelerator_0

# 3. Python 객체를 통해 하드웨어 제어
dma.sendchannel.transfer(input_buffer) # DMA로 데이터 전송
mlp.registers['CTRL'].write(1)         # 가속기 동작 시작
```

#### PYNQ 시스템의 구조

| 구성 요소 | 역할 | 설명 |
| :--- | :--- | :--- |
| **하드웨어 (PL)** | 고속 병렬 처리 | Vivado에서 Verilog/VHDL로 설계한 커스텀 로직 (AI 가속기 등). **AXI 인터페이스**를 통해 PS와 통신해야 합니다. |
| **소프트웨어 (PS)** | 시스템 제어 및 관리 | **리눅스 운영체제**가 실행됩니다. 그 위에서 PYNQ 프레임워크, 웹서버(Jupyter), Python 인터프리터가 동작합니다. |
| **Jupyter Notebook** | 사용자 인터페이스 | 사용자는 PC의 웹 브라우저를 통해 보드에 접속하고, Jupyter Notebook에 Python 코드를 작성하여 전체 시스템을 제어하고 테스트합니다. |

#### PYNQ를 사용했을 때의 장점 요약

1.  **빠른 프로토타이핑:** C/C++로 드라이버를 만들고 컴파일하는 과정 없이, Python으로 즉시 하드웨어를 테스트하고 결과를 확인할 수 있습니다.
2.  **하드웨어 추상화:** 복잡한 메모리 주소나 레지스터 맵을 외울 필요 없이, PYNQ 라이브러리가 제공하는 직관적인 API를 사용하면 됩니다.
3.  **풍부한 Python 생태계:** `Numpy`로 데이터를 분석하고, `Matplotlib`으로 결과를 시각화하며, `OpenCV`로 이미지를 처리하는 등 기존의 강력한 Python 라이브러리들과 FPGA 가속을 쉽게 연동할 수 있습니다.
4.  **활발한 커뮤니티와 자료:** PYNQ 공식 홈페이지와 커뮤니티 포럼에 다양한 예제와 완성된 오버레이들이 공유되고 있어, 이를 활용하거나 참고하여 개발 시간을 단축할 수 있습니다.

**결론적으로 PYNQ는 "하드웨어 가속의 성능"과 "Python 개발의 편의성"이라는 두 마리 토끼를 모두 잡을 수 있게 해주는 훌륭한 도구입니다.**

더 자세한 정보나 예제 코드가 필요하시면 PYNQ 공식 홈페이지([pynq.io](http://pynq.io))를 방문해 보시는 것을 강력히 추천합니다.

---

### 사용자 질문 4

그러면 ai가속기와 zybo7 보드는 vitis에서 소프트웨어적으로 코딩하고 / 데이터전처리 과정은 pynq를 이용하는데 그 전처리된 데이터를 vitis 환경에서 가져와서 ai가속기에 보내줄수있는거야 ?

---

### Gemini 답변 4

아, 그 부분에서 약간의 혼동이 있으신 것 같습니다. 아주 중요한 포인트입니다. 결론부터 말씀드리면, **Vitis와 PYNQ는 함께 사용하는 개념이 아니라, 소프트웨어 개발을 위한 두 가지 다른 경로(Path) 중 하나를 선택하는 것**입니다.

사용자님께서 질문하신 "PYNQ에서 전처리한 데이터를 Vitis 환경에서 가져와서 가속기에 보낸다"는 시나리오는 일반적인 개발 흐름이 아닙니다.

두 가지 개발 경로를 명확히 구분해서 설명해 드리겠습니다.

---

#### 경로 1: Vitis를 사용하는 C/C++ 기반 개발

이 경로는 전통적인 임베디드 시스템 개발 방식입니다.

1.  **하드웨어 설계 (PL):** Vivado에서 Verilog로 AI 가속기를 설계합니다. (이 부분은 두 경로 모두 동일)
2.  **소프트웨어 개발 (PS):** **Vitis IDE**를 사용합니다.
    *   **모든 소프트웨어 로직을 C/C++로 작성합니다.**
    *   즉, **데이터 전처리** (이미지 파일 읽기, 리사이징, 정규화 등)도 C/C++ 코드와 관련 라이브러리(예: `standalone` 드라이버, `xil_printf` 등 또는 리눅스 환경의 C 라이브러리)를 사용해 직접 구현해야 합니다.
    *   작성된 C/C++ 코드는 AI 가속기를 제어하고, DMA를 통해 데이터를 보내고, 결과를 받는 역할까지 모두 수행합니다.
    *   **PYNQ나 Python, Jupyter Notebook은 이 경로에서 전혀 사용되지 않습니다.**

*   **장점:** 성능이 최적화되고, 리소스 사용량이 적습니다. 최종 제품화에 적합합니다.
*   **단점:** 개발 과정이 복잡하고, C/C++로 이미지 처리 등을 구현하는 것이 Python보다 훨씬 번거롭습니다. 디버깅도 상대적으로 어렵습니다.

---

#### 경로 2: PYNQ를 사용하는 Python 기반 개발 (추천)

이 경로는 생산성과 편의성에 초점을 맞춘 현대적인 개발 방식입니다.

1.  **하드웨어 설계 (PL):** Vivado에서 Verilog로 AI 가속기를 설계합니다. (이 부분은 두 경로 모두 동일)
2.  **소프트웨어 개발 (PS):** **Jupyter Notebook**을 사용합니다.
    *   Zybo 보드에 PYNQ 이미지를 설치하여 리눅스+Python 환경을 부팅합니다.
    *   **모든 소프트웨어 로직을 Python으로 작성합니다.**
    *   **데이터 전처리**는 `OpenCV`, `Numpy` 같은 강력한 Python 라이브러리를 사용해 매우 간단하게 구현합니다.
    *   PYNQ 라이브러리를 사용하여 AI 가속기를 제어하고, DMA를 통해 전처리된 데이터를 보내고, 결과를 받는 역할까지 모두 Python 코드 내에서 수행합니다.
    *   **Vitis IDE는 이 경로에서 소프트웨어 코딩에 직접적으로 사용되지 않습니다.** (하드웨어 정보를 Vitis에서 빌드하여 PYNQ로 가져올 수는 있지만, 주된 개발 환경은 Jupyter입니다.)

*   **장점:** 개발이 매우 빠르고 편리합니다. 인터랙티브한 테스트와 디버깅이 가능합니다.
*   **단점:** C/C++에 비해 약간의 성능 오버헤드가 있을 수 있으나, 프로토타이핑 단계에서는 무시할 만한 수준입니다.

---

#### 결론

**Vitis(C/C++)**와 **PYNQ(Python)**는 AI 가속기를 제어하는 소프트웨어를 만들기 위한 **두 개의 독립적인 세계**라고 생각하시면 됩니다.

*   **Vitis를 선택했다면,** 전처리도 C/C++로 해야 합니다.
*   **PYNQ를 선택했다면,** 전처리도 Python으로 하고, 가속기 제어도 Python으로 합니다.

따라서 사용자님의 프로젝트에는 **경로 2 (PYNQ 사용)** 가 훨씬 적합합니다. Python의 편리함을 이용해 전처리를 하고, 그 결과물을 PYNQ 라이브러리를 통해 그대로 AI 가속기로 보내는 것이 가장 효율적인 방법입니다.

---

### 사용자 질문 5

그러면 카메라 모듈을 이용해서 보드에 정보를 받는다면 python로 할수있는거야 ?

---

### Gemini 답변 5

네, 물론입니다. **PYNQ(Python) 환경에서 카메라 모듈을 사용하는 것은 매우 일반적이고 강력한 기능**입니다.

오히려 PYNQ의 가장 큰 장점 중 하나가 카메라 같은 외부 장치와의 상호작용을 Python으로 쉽게 처리할 수 있다는 점입니다.

카메라를 연결하고 Python으로 데이터를 받는 방법은 크게 두 가지가 있습니다.

#### 방법 1: USB 웹캠 사용 (가장 쉬운 방법)

가장 간단하고 빠르게 시작할 수 있는 방법입니다.

1.  **하드웨어 연결:** 일반적인 USB 웹캠을 Zybo 보드의 USB 포트에 꽂습니다.
2.  **소프트웨어 처리 (PYNQ/Python):**
    *   PYNQ에 내장된 리눅스는 USB 웹캠을 자동으로 인식합니다 (UVC 드라이버 사용).
    *   Python의 **OpenCV 라이브러리**를 사용하면, 단 몇 줄의 코드로 카메라에서 이미지를 가져올 수 있습니다. PC에서 웹캠을 사용하는 것과 코드가 거의 동일합니다.

**Jupyter Notebook 예시 코드:**

```python
import cv2
import numpy as np
from pynq import allocate

# OpenCV를 통해 카메라 장치를 엽니다 (보통 0번 장치).
cap = cv2.VideoCapture(0)

# 카메라에서 한 프레임을 읽어옵니다.
# ret는 성공 여부(True/False), frame은 이미지 데이터(Numpy 배열)입니다.
ret, frame = cap.read()

if ret:
    # 여기에 이미지 전처리 코드를 작성합니다.
    # 예: 28x28 크기로 리사이즈하고 흑백으로 변환
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    processed_frame = cv2.resize(gray_frame, (28, 28))

    # 이제 'processed_frame' (Numpy 배열)을
    # AI 가속기로 보낼 수 있습니다.
    
    # ... (DMA를 통해 가속기로 데이터 전송하는 코드) ...

# 사용이 끝나면 장치를 해제합니다.
cap.release()
```

**이 방법의 장점:**
*   별도의 FPGA 하드웨어 설계(Verilog)가 필요 없습니다.
*   설정이 매우 간단하며, PC에서 테스트한 코드를 거의 그대로 사용할 수 있습니다.
*   **프로젝트 시작 단계에서 가장 추천하는 방법입니다.**

#### 방법 2: MIPI CSI-2 카메라 사용 (고성능 방법)

Zybo Z7 보드에는 Digilent의 Pcam과 같은 MIPI 카메라를 위한 전용 커넥터가 있습니다.

1.  **하드웨어 연결:** Pcam을 보드의 MIPI 커넥터에 연결합니다.
2.  **하드웨어 설계 (Vivado):**
    *   USB 방식과 달리, MIPI 카메라의 신호를 해석하기 위한 **FPGA 로직(IP)이 필요합니다.**
    *   Vivado에서 'MIPI CSI-2 RX Subsystem' 같은 IP를 블록 디자인에 추가하여, 카메라 데이터를 AXI-Stream이라는 표준 비디오 스트림으로 변환해야 합니다.
    *   보통 `MIPI RX IP -> VDMA(Video DMA) -> DDR 메모리` 와 같은 파이프라인을 설계합니다.
3.  **소프트웨어 처리 (PYNQ/Python):**
    *   PYNQ에는 이런 비디오 파이프라인을 쉽게 제어할 수 있는 `pynq.lib.video` 라이브러리가 내장되어 있습니다.
    *   이 라이브러리를 사용하여 VDMA를 설정하고 카메라로부터 프레임 캡처를 시작합니다.
    *   캡처된 프레임은 메모리상에서 Numpy 배열로 접근할 수 있으므로, 그 이후의 전처리 과정은 USB 방식과 유사합니다.

**이 방법의 장점:**
*   USB보다 더 높은 대역폭과 낮은 지연 시간(latency)을 제공합니다.
*   더욱 시스템에 통합된(FPGA-native) 솔루션입니다.

**단점:**
*   카메라 신호를 받기 위한 별도의 FPGA 설계가 필요하여 초기 설정이 복잡합니다.

#### 결론

**네, PYNQ(Python) 환경에서 카메라를 사용하는 것은 완벽하게 가능합니다.**

프로젝트를 처음 시작하신다면, **USB 웹캠과 OpenCV를 사용하는 방법 1로 시작**하여 전체 시스템(데이터 입력 -> 전처리 -> 가속 -> 결과 출력)의 흐름을 먼저 완성해 보시는 것을 강력하게 추천합니다. 그 후에 성능 요구사항에 따라 방법 2로 전환하는 것을 고려해볼 수 있습니다.

---

### 사용자 질문 6

그러면 pynq 사용할면 vivado 에서 설정을 어떻게해줘야돼 ?

---

### Gemini 답변 6

네, PYNQ를 사용하기 위한 Vivado 설정은 PYNQ가 하드웨어를 인식하고 제어할 수 있도록 **'정해진 규칙'에 따라 시스템을 설계**하는 과정입니다.

핵심은 **"모든 제어와 데이터 흐름은 AXI 버스를 통해 이루어져야 한다"** 는 것입니다.

PYNQ가 Python 코드로 하드웨어를 제어할 수 있는 마법의 비밀은, Vivado에서 설계된 하드웨어의 AXI 주소 맵을 PYNQ가 해석해서 자동으로 드라이버를 만들어주기 때문입니다.

#### PYNQ를 위한 Vivado 블록 디자인 체크리스트

Vivado에서 블록 디자인(Block Design)을 할 때, 다음 요소들이 필수적으로 포함되고 올바르게 연결되어야 합니다.

##### 1. Zynq Processing System (PS) 추가
*   가장 기본입니다. 블록 디자인에 **'ZYNQ7 Processing System'** IP를 추가합니다.
*   이것이 리눅스와 PYNQ 소프트웨어가 실행될 ARM 프로세서(두뇌)입니다.
*   IP를 추가한 후 상단에 뜨는 'Run Block Automation'을 실행하여 기본적인 PS-DDR 메모리 설정을 완료합니다.

##### 2. AXI 인터페이스 활성화
*   추가된 Zynq PS IP를 더블클릭하여 설정 창을 엽니다.
*   `PS-PL Configuration` -> `AXI Non Secure Enablement` -> `GP Master AXI Interface` 에서 **'M AXI GP0'** 와 **'M AXI GP1'** 포트를 활성화(enable)합니다. (하드웨어를 제어하기 위한 경로)
*   `PS-PL Configuration` -> `HP Slave AXI Interface` 에서 **'S AXI HP0'** 포트를 활성화합니다. (DMA가 메모리에 접근하기 위한 고속 경로)

##### 3. 제어용 버스: `AXI-Lite`
*   사용자가 Verilog로 만든 AI 가속기 IP가 있다면, 이 IP에는 반드시 **'AXI-Lite Slave'** 인터페이스가 있어야 합니다.
*   **역할:** Python에서 IP의 내부 레지스터를 읽고 쓰는 통로입니다. 예를 들어, "가속기 동작 시작" 신호를 보내거나, "가속기 동작 상태"를 읽어오거나, 특정 설정값(threshold 등)을 전달하는 데 사용됩니다.
*   **연결:** Zynq PS의 `M_AXI_GP0` (Master) 포트를 AI 가속기 IP의 `S_AXI_LITE` (Slave) 포트에 연결합니다.

##### 4. 데이터용 버스: `AXI-Stream` + `AXI DMA`
*   이미지 데이터처럼 대용량의 데이터를 AI 가속기로 보내기 위해서는 `AXI-Stream`이라는 표준을 사용합니다.
*   하지만 PS의 메모리(DDR)와 PL의 `AXI-Stream`은 직접 통신할 수 없기 때문에, 중간에서 데이터를 변환하고 옮겨주는 **'AXI Direct Memory Access (DMA)'** IP가 반드시 필요합니다.

*   **AXI DMA의 역할:**
    *   PS의 DDR 메모리에서 데이터를 읽어와서 AI 가속기에게 `AXI-Stream` 형태로 뿌려줍니다. (PS -> PL)
    *   AI 가속기의 `AXI-Stream` 출력 데이터를 받아서 PS의 DDR 메모리에 써줍니다. (PL -> PS)

*   **연결 (매우 중요):**
    *   **제어:** Zynq PS의 `M_AXI_GP0` -> AXI DMA의 `S_AXI_LITE` 포트 (DMA를 제어하기 위함)
    *   **데이터 (PS <-> DMA):** Zynq PS의 `S_AXI_HP0` <-> AXI DMA의 `M_AXI_SG` 및 `M_AXI_MM2S/S2MM` 포트 (DMA와 메모리 사이의 고속 통로)
    *   **데이터 (DMA <-> 가속기):** AXI DMA의 `M_AXIS_MM2S` -> AI 가속기의 입력 `S_AXIS` 포트, AI 가속기의 출력 `M_AXIS` -> AXI DMA의 `S_AXIS_S2MM` 포트

##### 간단한 블록 구조 예시:
```
+--------------------------+      M_AXI_GP0      +---------------------+
|                          |-------------------->| S_AXI_LITE (제어)   |
|   ZYNQ7 Processing       |                     |                     |
|   System (PS)            |      M_AXIS_MM2S    |   AI 가속기 IP      |
|                          | (Stream out)        |   (AXI-Stream)      |
|      +-----------+       |<--------------------|                     |
|      | S_AXI_HP0 |<----->| M_AXI_MM2S/S2MM     |                     |
|      +-----------+       | (Memory-mapped)     +---------------------+
|                          |
+--------------------------+      S_AXIS_S2MM
                             (Stream in)
                                   ^
                                   |
                             +-----+---------------+
                             | AXI DMA             |
                             +---------------------+
```

##### 5. 클럭 및 리셋 연결
*   위에서 연결한 모든 AXI 인터페이스(`ACLK`, `ARESETn`)에는 클럭과 리셋 신호가 연결되어야 합니다.
*   보통 Zynq PS에서 나오는 `FCLK_CLK0`와 `peripheral_aresetn`을 공통으로 사용합니다. 'Run Connection Automation' 기능을 사용하면 대부분 자동으로 연결해 줍니다.

#### 최종 산출물: 오버레이 파일 생성

이 모든 과정을 거쳐 블록 디자인을 완성하고 `Validate Design`을 통과했다면,
1.  **'Create HDL Wrapper'** 를 실행합니다.
2.  **'Generate Bitstream'** 을 실행합니다.

생성 작업이 완료되면 PYNQ에 필요한 두 개의 파일이 만들어집니다.

1.  **Bitstream 파일:** `프로젝트경로/프로젝트이름.runs/impl_1/디자인이름_wrapper.bit`
2.  **Hardware Handoff 파일:** `프로젝트경로/프로젝트이름.srcs/sources_1/bd/디자인이름/hw_handoff/디자인이름.hwh`

이 **`.bit` 파일과 `.hwh` 파일을 한 폴더에 넣고** Jupyter Notebook에 업로드하여 `Overlay` 클래스로 로드하면, PYNQ가 `.hwh` 파일을 분석해 위에서 설계한 모든 IP에 대한 Python 객체를 만들어주는 것입니다.

---

### 사용자 질문 7

그러면 ai 가속기도 저기 블록에 연결되어있어야 겠네 ?

---

### Gemini 답변 7

네, 정확합니다! **바로 그 점이 핵심입니다.**

사용자님이 Verilog로 작성하실 **AI 가속기는 Vivado 블록 디자인의 '부품' 중 하나로 포함되고, 다른 IP들과 버스로 연결되어 있어야 합니다.**

방금 보여드린 다이어그램에서 'AI 가속기 IP'라고 표시된 부분이 바로 사용자님의 Verilog 코드가 들어갈 자리입니다.

#### Verilog 코드를 블록 디자인에 포함시키는 과정

아마 "내가 짠 Verilog 코드를 어떻게 저 블록 그림에 넣지?" 라는 궁금증이 생기실 텐데, 그 과정은 다음과 같습니다.

1.  **AXI 인터페이스를 갖도록 Verilog 코드 작성:**
    *   AI 가속기 모듈을 설계할 때, 외부와의 통신 포트를 AXI-Lite(제어용)와 AXI-Stream(데이터용) 표준에 맞춰서 만들어야 합니다. Vivado에서 AXI 인터페이스 템플릿을 자동으로 생성해주는 기능이 있어 처음부터 다 만들 필요는 없습니다.

2.  **Vivado IP Packager 사용:**
    *   Vivado에는 사용자가 작성한 Verilog/VHDL 코드를 블록 디자인에 넣을 수 있는 'IP (Intellectual Property)'로 포장해주는 **'Create and Package New IP'** 라는 강력한 기능이 있습니다.

3.  **IP 패키징 과정:**
    *   이 기능을 실행하고, AI 가속기 Verilog 소스 파일을 추가합니다.
    *   마법사(Wizard)가 코드의 포트를 분석하고, 어떤 포트가 AXI-Lite 버스인지, 어떤 포트가 AXI-Stream 버스인지 등을 지정해달라고 요청합니다.
    *   정보를 입력하고 패키징을 완료하면, 사용자만의 **'커스텀 AI 가속기 IP'** 가 생성됩니다.

4.  **블록 디자인에 추가 및 연결:**
    *   이제 Vivado의 IP 카탈로그(IP Catalog)에 보면, AXI DMA나 Zynq PS처럼 사용자님이 만든 'AI 가속기 IP'가 목록에 나타납니다.
    *   이것을 블록 디자인 캔버스로 끌어다 놓습니다.
    *   그리고 앞에서 설명한 대로, Zynq PS와 AXI DMA에 AXI 버스로 선을 그려 연결해주는 것입니다.

**요약:**
**Verilog 코드 작성 -> IP로 패키징 -> 블록 디자인에 추가 -> 다른 부품들과 연결**

이 과정을 거쳐야만 PYNQ(PS)가 AI 가속기(PL)의 존재를 인식하고, AXI 버스를 통해 제어 신호와 데이터를 주고받을 수 있게 됩니다.
